# -*- coding: utf-8 -*-
"""Sentiment Analysis of Drug Review.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MyQJkGYT_wFLmeFzEsskqpK_RB87GYyW
"""

# =========================
# Drug Reviews Sentiment – Colab Ready
# =========================

# 0) Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, classification_report, ConfusionMatrixDisplay,
    roc_curve, auc
)
from sklearn.pipeline import Pipeline
from sklearn.utils import Bunch
import joblib

# 1) Load CSV (with headers already present)
PATH = "/content/drugsComTest_raw.csv"  # change if needed
df = pd.read_csv(PATH)

# Safety: trim accidental whitespace in headers
df.columns = df.columns.str.strip()

# Validate required columns
required = ["uniqueID","drugName","condition","review","rating","date","usefulCount"]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"Missing expected columns: {missing}\nFound: {df.columns.tolist()}")

print("Loaded:", df.shape, "columns:", df.columns.tolist())
print(df.head(2))

# 2) Basic cleaning
# Keep only rows with non-empty review and valid rating
df = df.dropna(subset=["review","rating"]).copy()
# Ensure rating is numeric (coerce bad entries -> NaN, then drop)
df["rating"] = pd.to_numeric(df["rating"], errors="coerce")
df = df.dropna(subset=["rating"])

# Optional: light text cleaning (don’t overdo; TF-IDF can handle most)
def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    # Replace common HTML entities
    s = (s.replace("&#039;", "'")
           .replace("&quot;", '"')
           .replace("&amp;", "&"))
    # Strip extra whitespace
    return " ".join(s.split())

df["review_clean"] = df["review"].apply(clean_text)

# 3) Label sentiment from rating:
#    <=4 negative (0), >=7 positive (1), drop neutral (5-6)
def rating_to_sentiment(x):
    if x <= 4: return 0
    if x >= 7: return 1
    return np.nan

df["sentiment"] = df["rating"].apply(rating_to_sentiment)
before = len(df)
df = df.dropna(subset=["sentiment"]).copy()
df["sentiment"] = df["sentiment"].astype(int)

print(f"\nAfter labeling and dropping neutrals (5–6): {before} -> {len(df)} rows")
print("Class balance:\n", df["sentiment"].value_counts())

# 4) Train / Test split (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    df["review_clean"], df["sentiment"],
    test_size=0.2, random_state=42, stratify=df["sentiment"]
)

print("\nTrain/Test sizes:", X_train.shape[0], "/", X_test.shape[0])

# 5) Build pipeline: TF-IDF + Logistic Regression
# Use class_weight='balanced' in case of class imbalance
pipe = Pipeline(steps=[
    ("tfidf", TfidfVectorizer(
        stop_words="english",
        max_features=30000,      # plenty for 53k rows
        ngram_range=(1,2),       # unigrams + bigrams
        lowercase=True
    )),
    ("clf", LogisticRegression(
        max_iter=2000,
        class_weight="balanced",
        solver="liblinear"       # robust for sparse TF-IDF
    ))
])

# 6) Train
pipe.fit(X_train, y_train)

# 7) Evaluate
y_pred = pipe.predict(X_test)
y_proba = pipe.predict_proba(X_test)[:, 1]

acc = accuracy_score(y_test, y_pred)
print(f"\n✅ Test Accuracy: {acc:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred, digits=4))

# Confusion Matrix
disp = ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred,
    display_labels=["Negative (0)", "Positive (1)"]
)
plt.title("Confusion Matrix")
plt.show()

# ROC Curve (binary)
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, label=f"LogReg (AUC={roc_auc:.3f})")
plt.plot([0,1], [0,1], linestyle="--", label="Chance")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid(True)
plt.show()

# 8) Example inference helper
def predict_reviews(reviews):
    """
    reviews: list of raw review strings
    returns: DataFrame with text, prob_positive, label
    """
    cleaned = [clean_text(r) for r in reviews]
    probs = pipe.predict_proba(cleaned)[:,1]
    labels = (probs >= 0.5).astype(int)
    return pd.DataFrame({
        "review": reviews,
        "prob_positive": probs,
        "pred_label": labels
    })

# Demo predictions
demo = [
    "This medicine worked wonderfully, no side effects.",
    "Terrible headache and nausea, would not recommend."
]
print("\nDemo predictions:")
print(predict_reviews(demo))

# 9) Save artifacts (optional)
joblib.dump(pipe, "/content/drug_review_sentiment_pipeline.joblib")
print("\nSaved pipeline to /content/drug_review_sentiment_pipeline.joblib")

